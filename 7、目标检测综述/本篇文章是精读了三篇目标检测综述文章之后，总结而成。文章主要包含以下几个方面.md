- **目标检测的发展过程(时间轴顺序)**
- 传统检测算法
    - V.J 检测器
    - HOG
    - DPM
  
- CNN的Two Stage算法
    - RCNN
    - SPPNet
    - Faster RCNN
    - Feature Pyramid Network
  - CNN的One Stage算法
    - YOLO系列
    - SSD系列
    - RetinaNet系列
- **目标检测的技术发展**
- 早期的原始方法
  - 多尺度检测 Mutil-Scale
  - BBox回归
  - NMS技术发展
  - 困难样本挖掘
- **检测算法加速**
- 特征图共享计算量
  - 分类器加速
  - 级联检测器
  - 网络剪枝和量化
  - 轻量化网络设计
  - 数值计算加速
- **目标检测最新进展**
- 更好的主干网络
  - 提取更好的特征
  - 滑窗算法升级 sliding windows
  - 提升定位精度
  - 带分割信息训练
  - 旋转和多尺度适应检测
  - 从头训练(Training Form Scratch)
  - 对抗训练
  - 弱监督学习目标检测
- **落地应用**
  - 行人检测
  - 人脸检测
  - 文字检测
  - 交通标志检测
  - 医学图像识别
  - 遥感图像目标检测
- **总结与未来趋势**
  - 轻量级的目标检测
  - 目标检测遇上AutoML
  - 视频目标检测
  - 异质数据融合目标检测

---

#### 一、目标检测的发展过程

![目标检测发表数量](https://i.loli.net/2019/12/21/r13WgmFoHPwBQf7.png)

上图是从1998年到2018年，目标检测文章发表数量变化图，数据来源于谷歌学术。由此可见目标检测领域一直是大家所追捧的热方向。

![](https://i.loli.net/2019/12/21/n94vwIbu867aByh.png)

上图展示了目标检测算法近20年来来的方法路线图。很明显，从2012年(深度学习元年)开始，深度学习发展的步伐越来越大。图中每一个标注出来的方法名字都是具有里程碑意义的算法。

**1.1传统算法**

- **V.J Detector**

  19年前，P. Viola 和 M. Jones提出一个新的检测算法，主要应用在人脸检测方面，运行在主频为700MHz的奔腾CPU上，比当时其他的算法速度提升了上百倍。

- **HOG Detector**

  在2005年被提出，因为其特征鲁棒性强，多尺度适应性好，在深度学习出现之前，经常被广泛应用于通用目标检测。

- **DPM**

  DPM是传统算法的**巅峰**，是VOC-07，08，09目标检测比赛的赢家，它是HOG方法的拓展。尽管现在的目标检测算法远远强过了DPM，但是DPM提出的很多东西，现在都在沿用，例如难例挖掘，Bbox 回归。所以其作者被VOC颁发了“终身成就奖”。

**1.2 Two-Stage算法**

- **RCNN**

  RCNN由SS算法(selective search)得到proposals，然后每一个proposal被送到CNN中提取特征，最后有SVM分类器去预测目标种类，RCNN将VOC07的mAP从33.7%(DPM保持的)上升到58.5%。

- **SPPNet**

  进一步提升精度，从58.5%到59.2%，并且其推理速度相比SPPNet快了20倍

- **Fast RCNN**

  VOC07精度提升到70.0%，然后其推理速度相比SPPNet又快了10倍

- **Faster RCNN**

  首次可以将two-stage的网络进行end2end的训练，并且在VOC07上精度达到73.2%，同时其运行速度达到了几乎实时。

- **FPN**

  2017年在Faster RCNN基础上提出FPN，在COCOmAP@.5上达到59.1%的精度。

**1.3 One-Stage算法**

- **YOLO**

  YOLO在2015年被首次提出，是深度学习领域的第一个One-Stage的目标检测算法，在VOC07上精度在52.7%的同时，速度可以达到155fps，可谓逆天！由于精度原因后来发布了YOLOV2，其成绩为45fps with VOC07 mAP=63.4%，后来在2018年发布了YOLOV3，吊打同期目标检测方法，直到现在YOLOV3的方法仍然不过时。

- **SSD**

  SSD方法是在2015年被提出来的，它是深度学习领域第二个One-Stage的检测器。同时兼顾了速度和精度，对后面的目标检测算法有着深远的影响。其成绩为(VOC07 mAP=76.8%, VOC12 mAP=74.9%, COCO mAP@.5=46.5%, mAP@[.5,.95]=26.8%),基于SSD的方法的目标检测算法非常地多。

- **RetinaNet**

  FocalLoss在这篇文章被提出来，主要解决的是类别不平衡的问题。FocalLoss使OneSatge的方法在精度上可以和twoSatge的方法进行抗衡了。(COCO mAP@.5=59.1%, mAP@[.5, .95]=39.1%).

![accuracy](https://i.loli.net/2019/12/21/wQ7mWDKfZ6j2qxP.png)

如上图所示，可以看到上面所列举出来的一系列算法在不同数据集上精度的变化。上面所列出来的每一个算法都是具有里程碑意义的，都值得深入去研究

#### 二、目标检测中的技术发展

**2.1 早期传统方法**

在2000年之前，没有一个统一的检测哲学，检测器通常都是基于一些比较浅层的特征去做设计的，例如组成，形状，边界等等。后来基于机器学习的检测方法发展了一段时间，例如外观统计模型，小波特征表示，梯度表示。在这里就不展开讨论早期的方法了。

**2.2早期的CNN做目标检测**

最早在1990年，杨乐春(Y.LeCun)就已经开始使用CNN做目标检测了，只是由于当时的硬件计算力水平不行，所以导致设计的CNN结构只能往简单的去设计，不然没有好的硬件去运行完成实验。所以说我一直认为深度学习有今天的成就，很大一部分要归功于英伟达。当时做过一些列的提升性的实验，例如“shared-weight replicated neural network”，这个很像我们现在的全卷积网络FCN。

<img src="https://i.loli.net/2019/12/21/IVixqmLEOsupTto.png" style="zoom:50%;" />

**2.3 多尺度检测的技术发展**

目标检测技术两大拦路虎就是**多尺度目标**、**小尺寸目标**，这两个就是目标检测技术发展到现在一直在解决的问题。下图所示的是多尺度检测的技术发展路线图

![](https://i.loli.net/2019/12/21/4k687qDsKiHMYVb.png)

多尺度检测经历了好几个阶段

- **在2014年以前是特征金字塔加上滑窗**

  这个不展开写，感兴趣的去了解下 VJ算法。

- **在2010-2015是先使用object proposal然后检测**

  这种方法一般是将检测分为两个部分，第一个部分先做推选框，第二部分是根据推选框做进一步的分类，基于推选框的算法，一般有以下几个特征：1，召回率比较高；2，时间消耗比较大。在2015年以前，检测算法如果有比较好的精度，基本上都是TwoStage的。

- **在2013-2016年，直接OneStage的deep regression，yolo最具有代表性；**

  随着GPU运算能力的增强，人们对待多尺度目标检测变得越来越直接，越来越**暴力**，因为使用deep regression的方法去做多尺度检测，这种方法思路很简单，直接使用深度学习的特征去得到BBox的坐标，非常的暴力，类似于MTCNN的单阶段网络的思想。**暴力训练，暴力回归**。其优点很明显，思路简单，只要有GPU就可以复现，但是也有缺点，那就是定位精度不高，尤其是小目标。

- **2016年以后，就是Multi-reference/-resolution detection**

  对于检测多尺度目标，目前最流行的方法还是Multi-reference，其主要的思想就是预先定义一组reference boxes，例如经常用的anchor box，它们具有不同的尺寸和缩放因子，然后检测器基于这些boxes，去做运算。这种类型算法的loss一般是由两部分组成，第一个是定位loss，另一个是分类loss，两者加权求和之后就是最后的目标检测的loss，如下图。做分类的loss，一般都是交叉熵，定位的loss一般是L2，两者加权求和。

  ![](https://i.loli.net/2019/12/21/rsyZa1wB5TEDJex.png)

**2.4 Bounding Box Regression的技术发展**

bbox回归对于目标检测的定位精度的提升至关重要，它主要是为了修正基于proposals的bbox的位置。

![](https://i.loli.net/2019/12/21/92OhreEQ6zDuXRV.png)

如上图所示，BBOX回归也经历了几个阶段

- 无BBox回归

  在早期的检测算法中，都是不使用bbox回归的，直接使用滑窗的方式来定位。

- 从特征图得到BBBox

  自从fasterRCNN之后，BBox回归不再是一个单独的程序，而是直接可以集成到CNN中进行端到端的训练的，所以才会有从特征图到BBox。例如fasterRCNN的smooth-L1函数

  <img src="https://i.loli.net/2019/12/21/W1eb6Q2nFuk4qZx.png" style="zoom:50%;" />

**2.5 NMS的技术发展**

nms是一个非常重要的技术手段。如果对于有同一个目标上出现多个检测的框的时候，NMS可以根据每个框的score来进行优化，去除掉一部分的多于的框。

![](https://i.loli.net/2019/12/21/F2s4d6zwkSUqxgZ.png)

nms有以下三种

![](https://i.loli.net/2019/12/21/hItS78V9Gj2YuTb.png)

- Greedy selection

  这是一种具有很悠久历史的nms方法，也是目标检测中应用最普遍的方法。首先对检测器检测到的box根据confidence的得分进行排序，然后分别计算所有的box的相互之间的iou值，然后设置一个阈值，如果高于设置的阈值，则保留confidence高的框，舍弃confidence低的框，以此类推。

- Learning to NMS

  这种方法的思路是nms的阈值也应该是属于网络训练的一个参数，不能固定的设置为定值。例如有一个文章叫做 soft NMS，仅仅就是将nms算法进行修改，在faster RCNN上应用就可以在测试集上涨点。将nms的阈值也设置为科学习的，在另一个角度上更加符合end2end的思想，理论上也是更优秀的。

**2.6 困难样本挖掘的技术发展**

困难样本是什么意思？ 在目标检测深度学习的训练过程中，正负样本的比例其实不均衡的，因为标注的数据就是正样本，数量肯定是固定的，为了保证正负样本的均衡，所以会选取一定的背景图片作为负样本，但是背景图片的样本集是一个open-set，不可能全部参与训练。所以需要将训练过程中难以训练的样本挖掘出来，给以更高的loss来训练，促进模型的泛化能力。

![](https://i.loli.net/2019/12/21/HRMdKB2nuiZqQ6m.png)

例如SSD算法中，仅仅是将一小部分的样本做反向传播，这些小部分样本都是loss值比较高的，这就是证明不好训练的样本，多学习几次，增加模型的泛化能力。

#### 三、目标检测的加速

一个算法从被设计出来，其目的就是为了产生价值的，并不是停留在学术界的文章中，必定是要被工业界拿来项目落地的。所以一个目标检测算法的推理时间对于工业界落地至关重要。

![](https://i.loli.net/2019/12/21/tUoYdbanrHgqTEN.png)

算法的加速，归根结底就是计算机的运算次数变少，但是精度不能丢。常用的几种加速方法如下

- 特征图共享计算

  在目标检测算法中，特征提取阶段往往耗时往往最多。在特征图共享计算里面分为两种，第一种是空间计算冗余加速，第二种是尺度计算冗余加速。这里不展开讲。

- 分类器加速

  早期目标检测中，是提取特征加上分类器这样一个套路来进行目标检测的，分类器一般是线性分类器，但是线性分类器没有非线性分类器效果好，例如svm就是非线性的，所以加速分类器的运行也是提升检测算法速度的一个方法。

- 级联检测器

  级联检测器可以很好的将计算耗时固定在一个比较小的范围，采用多个简单的检测，然后将其级联，从粗到细的过滤，例如cascade haar，MTCNN都是级联结构的

- 网络剪枝和量化

  网络剪枝和量化是目前模型压缩中非常常用的两种方法，也是做加速非常热门的方向。

  - 剪枝的意思就是在原来网络结构的基础上，对于一些网络结构进行修剪，在尽量不影响精度的前提下降低网络的计算量，例如减少通道数，合并网络层参数等等。
  - 网络量化的常用操作就是将原来浮点型的计算量化为定点运算，甚至于变为与或运算，这样大大降低网络的运算量，并且可以非常方便地部署到嵌入式设备上。
  - 网络蒸馏的意思就是将一个比较复杂的网络的学习到的“知识”蒸馏出来，“教给”一个比较小的网络学习，这样小网络的精度比较高，运算耗时也比较小。通俗的理解就是“老师网络”指导“学生网络”训练，然后最后用“学生网络”来部署应用。

- 轻量级网络设计

  轻量级网络设计是目前最热门的加速方式，我们常见的mobileNet的设计就是这个轻量级网络设计的典型代表。这里也有几种常用的方法

  - 分解卷积，将大卷积核分解为几个小的卷积核，这样其运算参数量就会降低。例如一个7x7的卷积核可以被分解为3个3x3的卷积核，它们的感受野相同，计算量后者要小，例如一个kxk的卷积核可以被分解为一个kx1和一个1xk的卷积核，其输出大小也相同，计算量却不同
  - 分组卷积，在早期硬件显存不够的情况下，经常用分组卷积来进行降低计算量，将特征通道分为不同的n组，然后分别计算
  - Depth-wise Separable Conv，深度可分离卷积，首次是mobileNet中提出来的，大大降低了卷积过程中的计算量。将普通卷积的深度信息分离出来，然后再利用1x1卷积将维度还原，即降低了计算量又在一定程度上使得特征图的通道重组，加速效果非常好
  - Bottle-neck Design，经常被用在轻量级网络的设计上，例如mobileNetV2就使用了反瓶颈层去设计网络。
  - Neural Architecture Search，简称NAS，从2018年AutoML问世以来，NAS发展非常的火，这种小型的网络结构是被训练自动搭建出来的。给机器限定一个搜索空间，让机器自己学习搭建一个高校的网络，总目前的效果来看，NAS搜索出来的结构比人工搭建的网络更加高效。例如mobileNetV3，efficientNet。

#### 四、目标检测最新进展

**4.1 使用更好的引擎**

检测器中非常重要的一个部分就是特征提取的主干网络，如果backbone性能优秀，检测器效果也会不错。例如FasterRCNN，SSD，RFCN其主干网络都是VGG或者resnet，如果对推理时间有要求，一般选取轻量级的网络作为主干网络，例如mobileNet-ssd就是mobileNet作为主干网络的SSD检测算法。所以说主干网络对时间的精度的影响非常大。

- VGG，在2014年被提出，有两种结构，分别是16层和19层，分别被称为VGG16和VGG19。VGG网络中使用3x3的卷积代替了5x5和7x7。
- GoogleNet，顾名思义这个网络由谷歌提出，将网络层数增到了22层，同时在网络中增加了BN层使得训练更加容易收敛
- Resnet，残差网络结构，在2015年被提出，其结构定制化程度非常高，从10层到152层都可以搭建，主要是解决了网络训练退化的问题，加入残差架构之后网络不会随着层数增加而产生退化现场。
- DenseNet，在残差网络的基础上进行修改，残差是有short cut链接，而denseNet块是前面所有的层都与后面层有链接，所以是稠密链接。

**4.2 使用更好的特征**

特征表达的质量对于目标检测也是起到关键性作用的，目前一些文章都在寻找一种更好的图像特征表达方式。例如特征融合，高分辨率特征。

同变性（Equivariance）和不变性（Invariance）是图像特征表达的两个重要指标，同变性在学习语义信息表示的时候非常重要，但是在目标定位的时候不变性又变得非常重要，所以往往需要进行特征融合。

在包含一系列的卷积，池化的深层CNN中，深层的特征往往具有很强的不变性，缺少同变性，尽管这样可以获得很好分类效果，但是定位精度就会损失。很容易理解，那就是浅层的特征虽然学习到的语义信息很少，但是它可以帮助定位，因为他包含了很多关于边界，轮廓的信息，所以需要将浅层特征和深度特征进行融合

特征融合一般有两种方法，第一种是Processing ﬂow，第二种是Element-wise operation。

- Processing ﬂow，类似于SSD的架构那种，将不同层次上的特征图进行融合，以适应不同大小目标的检测，使用跳跃链接引出然后融合特征
- Element-wise operation，此种方法非常简单，就是将特征图中的每一个元素进行简单的相加，相乘操作，暴力的进行融合

还有另外一种更好的特征表达方式，那就是增大特征图的分辨率，也就是说特征图在原图上有着更大的感受野，这样对于检测能力也有非常的提升。

**4.3 不止于滑窗**

很多方法都是基于proposals或者anchor的方法来做目标检测，但是最近非常流行anchor free，也就是将目标检测看成是关键点检测的问题，因为一个目标可以被表示为左上角和右下角的坐标包围的矩形框，所以这类问题可以被转换成不依赖于anchor的定位问题

**4.4 目标定位能力的提升**

提升定位能力，一般有两种方式

- 重新修正bbox，bbox refinement经常被用来在cnn中新加入一个分支来重新定位Bbox的位置。
- 重新设计损失函数，因为目前大部分的损失函数设计都是通过计算IoU来得到定位的loss，这样对于end2end的思想还是相差的有点远，如果能够重新设计一个loss函数来更好的表示定位误差，这样训练过程会更加的好。

**4.5 带语义监督信号学习**

在训练过程中，我们标注的都是矩形框，矩形框中或多或少都会标有一部分背景信息，如果没有语义信息，那么这种训练其实是不完美的。甚至于有些目标的外形比较奇怪，例如一个猫和一个非常长的火车，如果计算IoU的话，这样计算结果就不能很好的表示定位误差。如果带有语义信息的训练，然后使用多任务的损失函数，这样可以帮助到网络进行很好的学习。

**4.6 Training from Scratch**

现在的深度学习的网路训练之前，都是将主干网路在imageNet上进行预训练的，因为ImageNet数据集足够大，预训练之后再在目标检测的数据集上进行微调，这样会容易收敛。但是ImageNet是分类任务的，可能会和目标检测适应的不是那么好。最近有文章研究怎么去重头开始训练并提升精度，例如DSOD就是一个非常好的例子。

**4.7 对抗训练**

将GAN思想应用到目标检测中，特别是可以提高小目标和重叠目标的检出率，通过缩小小目标和大目标之间的表达范围，对抗网络生成拥挤的对象掩模，直接在特征层缩小拥挤程度，造成对抗攻击，提升网络学习能力。

**4.8 弱监督学习**

目标检测的数据集整理非常耗费人力去标注，通过弱监督学习训练目标检测，只需要在图片层面进行标注，不需要bbox层的标注就可以训练一个目标检测器

#### 五、目标检测的行业应用

-  行人检测
- 人脸检测
- 文字检测
- 交通标志检测
- 医学图像识别
- 遥感图像检测