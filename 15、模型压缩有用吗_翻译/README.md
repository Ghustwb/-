## 我们真的需要模型压缩吗

本文是译文，略有修改，[原文链接](http://mitchgordon.me/machine/learning/2020/01/13/do-we-really-need-model-compression.html)

模型压缩是一种可以减小神经网络计算开销的技术，被压缩的模型在使用少量计算资源的同时，其性能通常与原始模型相似。在实际应用中，其应用瓶颈在于训练原始的大型神经网络

**为什么要进行模型压缩？**

神经网络往往是过参数化的，存在很多参数冗余，不利于移动端部署

大算力的计算平台往往非常贵，模型压缩可以直接节省硬件成本



**适当参数化的模型**（Appropriately-Parameterized Models）

适当参数化模型指的是，模型参数既没有过参数化也没有欠参数化，而是参数量恰到好处，可以表示为任务的理想解决方案

在深度学习模型的训练过程中，不能直接训练出适当参数化的模型，因为对于给定数据集而言，是无法知道其参数量具体是多少才是合适的，即使知道了适当参数的解决方案，使用梯度下降法训练合适参数化的模型仍然非常困难

我们通常的套路如下图所示

![截屏2020-02-04下午12.47.42](/Users/lcgcf/Desktop/截屏2020-02-04下午12.47.42.png)

1、训练一个过参数化的模型

2、使用正则化技术，抑制训练过程中的过拟合

3、模型压缩通过消除冗余，在大模型中提取出“简单”模型，使内存和时间效率更加接近理想的适当参数化模型

极端的过参数化设计，可以使训练过程变得非常简单，但是极端的过参数化模型训练过程中，模型可以直接“记住”数据本身而不是数据中的有用信息，这也就是我们常说的训练过拟合。模型压缩就可以只保留实际解决方案中所需要的有用参数

我们还有一个目标是使用更少的GPU资源训练神经网络，所以一下几个问题也得解决

- 为什么过参数化是必须的？
- 过参数化的“过”程度如何把握？
- 可以使用更加智能的优化方法去降低过参数化吗？



**过参数化的界限**

为什么过参数化是必须的？

[Gradient Descent Finds Global Minima of Deep Neural Networks](http://arxiv.org/abs/1811.03804)和[Global Optimality in Neural Network Training](In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7331–39)这两篇文章指出，大模型能够使得损失函数更加地接近于凸函数，有利于进行优化求解。过度参数化设计减少了优化计算复杂度但是增加了内存消耗

尽管我们可以使用足够数量的参数来完美拟合某些数据，但我们仍然不知道要完美拟合数据所需的最小参数数量，过参数化的严格界限可能取决于优化过程的方法（例如SGD），计算这个过参数量的界限可能比训练所有可能的候选网络更加棘手。



**更好的优化方法**

从经验上讲，适当参数量的模型一般很难训练。使用梯度下降法训练一个参数量适当的模型，收敛情况将会非常不乐观，模型很难拟合训练数据。这部分的数学解释可以参考神经网络的优化环境的非凸性部分

模型压缩技术给了我们一个启示，通过阐述过参数量模型的趋向性类型，来指导训练合适参数量模型的训练。下面是一些模型压缩的类型

- 将很多权重设置为0，Pruning
- 权重矩阵低秩化
- 用更低的精度表示权重，量化
- 权值共享



**稀疏网络**

权重剪枝可能是模型压缩方法中提升优化能力的一种非常成功的技术，训练好的神经网络有30%-95%的权重是趋近于0的，这些权重可以再不影响网络精度的前提下被移除

![截屏2020-02-04下午3.55.16](/Users/lcgcf/Desktop/截屏2020-02-04下午3.55.16.png)

我们是否可以通过从一开始就训练稀疏神经网络来减少GPU使用，而不是事后修剪？

答案是否定的，因为稀疏网络也是非常难以训练，并且优化函数也是非凸的。

但是，[Frankel and Carbin ](http://arxiv.org/abs/1912.05671)朝着这个方向迈出了第一步，他们发现可以重新训练被剪枝过的网络，但前提是必须将其重新初始化为原训练期间使用的初始分布

![截屏2020-02-04下午4.01.17](/Users/lcgcf/Desktop/截屏2020-02-04下午4.01.17.png)

他们直接使用稀疏的网络使用随机初始化参数进行训练的时候，发现始终都无法得到一个稳定的解。只有当网络的初始化参数与剪枝之前的网络完全相同的时候才能训练成功。即模型训练是否成功跟参数的初始化有很大的关系

其他类型的模型压缩可能是重复这种模式，以下三种模式也许会更加普遍

1、模型压缩方法揭示出了训练神经网络中的冗余参数的常见规律

2、可以揭示出正则化，归纳偏置，方差与冗余的关系

3、找出一种新的优化方法，能够从头开始训练，不会出现参数冗余



下表列出了其他类型的模型压缩