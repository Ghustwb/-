**目标检测算法 CNN主干网络**

![SSD](https://i.loli.net/2019/12/26/mTMPVwp16oKLzER.png)

在深度学习目标检测算法中很重要的一个部分就是特征提取,用来提取特征的就是CNN中的backbone，Backbone的复杂度很大程度上决定了目标检测算法的耗时。 

例如我们常说的SSD算法就是Backbone为VGG16的SSD，后来由于工程化场景限制，衍生出一系列兼顾适应场景精度和速度的网络。例如[MobileNet-SSD](https://github.com/chuanqi305/MobileNet-SSD)，Resnet-SSD，[RefineDet-SSD](https://github.com/sfzhang15/RefineDet)

例如我们常说的YOLO，其Backbone是darkNet，后来也有同学做出了[MobileNet-YOLO](https://github.com/eric612/MobileNet-YOLO)

R-CNN的文章表明，将大规模的多尺度的分类问题中训练的模型作为目标检测的预训练模型,可以为训练检测器提供更丰富的语义信息，且可以提高检测性能。 在后来的几年中，这种方法已成为大多数对象检测器的默认策略。 

在此，总结一下在目标检测算法中常用的一些CNN Backbone.

**开始**，[VGG16](https://arxiv.org/abs/1409.1556)是在AlexNet基础上发展来的，其结构是直筒型，由五组卷积层和三个全连接层组成，在每一个组中使用最大值池化去降低空间维度，VGG16提出一个观点：通过堆加卷积层增加网络的深度，可以提高模型的表达能力。这个观点在当时一度很流行，但是后来人们才发现，如果使用SGD去训练网络，当网络深度到达一定数量之后（20层），模型训练和优化会遇到问题，“梯度弥散”导致深层的网络反而会比浅层的网络表现更差，这就被称为“网络退化”。所以直筒型的网络结构，也不可能无限制的堆积下去。

所以在2016年，[Resnet](https://arxiv.org/abs/1512.03385)被提出，主要是在网络中增加shortcut连接，降低训练优化难度。在resnet中有一个层可以直接跳过非线性层直接将数据传递到下一层。所以特征图可以被看做成浅层的激活和残差函数的求和，这样深层网络向浅层网络反传梯度的时候就像有了一条高速公路，所以再深的网络都不怕梯度弥散了，常见的resnet有16层到152层。

![](https://i.loli.net/2019/12/26/T3RAMoiXJgQGvk8.png)

**后来**，何恺明又提出一个resnet的预激活变体(pre-activation variant of resnet)，被称为[resnetV2](https://arxiv.org/pdf/1603.05027.pdf)，其实我们现在常说的resnet就是这个V2版本。作者的实验显示，合适的BN层的顺序可以提高原始Resnet的表现，这样一个非常简单且有效的改进直接使得resnet的层数可以达到1000层，增加深度的同时还有精度提升。

![resnetV2](https://i.loli.net/2019/12/26/9wIYJtG23WUXuNS.png)

**再后来**，又有个人觉得resnet还可以改进，他认为resnet没有全部利用好浅层的特征，导致参数量有些浪费，所以他提出了[DenseNet](https://arxiv.org/pdf/1608.06993.pdf),在2017年的CVPR直接被评为best paper。作者在这篇文章依然保持着ShortCut，同时做了一个疯狂的举动，把所有层都连接起来了，如下图所示。主要优点就是：增强特征传播，鼓励特征复用，更少的参数量。

![](https://i.loli.net/2019/12/26/QLCmqosy4KZ5IDj.png)

**再后来**，何恺明大神离开了MSRA，加入了FaceBook，和RBG大神混到了一起，又搞出了一个[ResNeXt](https://arxiv.org/abs/1611.05431),其主要思想是将分组卷积(Group Convolution)引进到了resnet中。101层的ResNeXt精度可以和ResNet相当，但是计算量只有后者的一半。从这个时间节点可以看出来，网络的精度再往上提升，如果按照现有的套路走确实很困难了，得另辟蹊径。所以很多学者的目标就转为研究保证精度不掉，降低计算量。

![](https://i.loli.net/2019/12/26/GVnQ4EocPFHfbdw.png)

**再后来**，谷歌提出[MobilNet](https://arxiv.org/abs/1704.04861)系列，截止到目前，MobileNetV1，V2，V3都已经问世。MobileNetV1的文章据说成文很早，但是发表出来的时间较晚。从MobileNetV1的结构中就可以看到依然是直筒型的，只是采用了DepthWise代替了传统的卷积，就已经将参数量优化了许多，并且精度也相当。将CNN部署到移动设备的进程又推进了一步。V2采用了"倒瓶颈层“的网络结构，不再是直筒型。V3是采用NAS技术搜索出来的最优结构。

![](https://i.loli.net/2019/12/26/W1uXBrSwmGH8Qyt.png)

上面所列举出来的网络都是常见的用作目标检测算法的BackBone，但是别忘了它们设计之初都是用来做分类任务的，所有的表现性能都是在分类测试集上得到的分数，用它们来搭建目标检测算法之后的预训练模型也是在ImageNet这种分类数据集上训练的。直接将分类模型的权重迁移到目标检测上来，其实不是最优的，因为目标检测和图片分类有一个潜在的冲突：

- 分类算法要求有很大的感受野，更多地关注空间不变性(spatial invariance),所以有很多的下采样操作被用来降低特征图的分辨率，所以最后生成的的特征图具有三个特征，低分辨率，空间不变性，大的感受野。但是在目标检测中，高分辨率的空间信息对于准确的定位目标具有很重要的作用。
- 分类只需要在单一的特征图上做预测就可以了，但是检测需要在不同表达能力的特征图上检测多尺度目标

**后来**，[DetNet](https://arxiv.org/abs/1804.06215)被提出来，用来平衡目标检测和分类网络的之间的冲突。使用了膨胀卷积(dilated convolutions)去增大感受野，DetNet也是在分类数据集上预训练，用来目标检测的BackBone。

**总结一下**：目标检测中为了提升精度而改进主干网络的方法最近变得越来越少了，很多文章都转向去研究怎么减少计算量不降低精度，从而更加适应工程部署。所以我推荐，刚开始做目标检测的同学们，去学习一下经典的网络结构和原理就好了，把重点放在研究LightWeight CNN上，不是花时间去研究怎么刷榜，刷榜对于现在来说意义不大了。

