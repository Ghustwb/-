未来AI，三分天下--算法、算力、终端

AI落地的矛盾始终在于资源和性能的平衡，在有限的资源下使性能最大化永远是项目落地的痛点

三分天下其一，算法已经由巨厂的巨佬们做的门槛非常低了，例如detectron，glounCV，里面做好了很多的算法，甚至于很多都是SOTA，对于我们从业者来说，仅仅是做个选择，看哪一种算法在资源上是balance的。

其二，算力这东西只能有芯片厂商去做。我敢说要是没有英伟达对GPU算力的贡献，深度学习的理论发展都要比现在慢3到5年。举个栗子，如果没有算力支持，学术界的科学家们从做实验到得到实验数据，这个周期都会变得无比漫长，自然而然拉长了整个发展进程。

其三，终端优化。在这个方面才是我们大众算法从业者一展拳脚的地方，该方向其实还可以分为理论和工程两个部分，理论可以理解为模型优化，工程可以理解为模型移植。

例如，Boss给了一个任务，要求在gtx1060显卡上跑行人检测，达到30FPS，让算法狗A和算法狗B一起做。刚好WeiLiu巨佬提出了一个叫做SSD的算法，狗A和狗B就决定使用SSD算法在1060上跑起来达到30fps。狗A直接运行SSD，发现根本跑不到30FPS啊，狗A是老司机，早就轻车熟路了，一般就将Backbone进行裁剪一下，当然会有精度损失，裁剪完了之后发现时间消耗还是很多，于是再仔细研究boss的需求和算法原理，咦，boss只要求检测行人，而行人一般是竖形的目标，且长宽比大约1:3，但SSD中priorBox的预推选框可是有3:1的，这里的计算是否有些冗余呢？能不能优化掉？然后这里优化差不多了，但是推理时间还是有超出，于是狗A对狗B说，我已经尽力了，你在部署的时候再想想办法吧。狗B也是老司机啊，在英伟达的gtx平台上部署，部署框架也就是TensorRT了，没得跑了啊，然后上TensorRT，一顿操作，转模型，计算加速，多线程。部署之后发现推理时间还是有一点超出，于是研究TensorRT，发现可以计算量化，原始模型计算是fp32，可以量化到fp16或者int8，于是又是一顿操作，直接干到int8试试，发现这推理速度逆天呀，这也太快了吧，但是这检出率也低了吧，精度降得厉害，“一顿操作猛如虎，一看精度0.5”。然后int8的量化估计用不了了，再换成fp16试试，改完之后发现时间和性能比较平衡了，那就这样吧，上线吧。后期再继续迭代。

上面举的这个例子，也就是一般算法工程师的套路了。当然很多公司狗A和狗B其实就是同一个人，同一个人做下来效率其实会更高一些。当然，里面的流程是这个流程，但是细节可能不完全是这个细节，仅仅举个栗子。

下面列举一些工程中终端优化常用的技术手段

1、模型优化

- 模型压缩
  - 模型重构
    - 知识蒸馏(teacher-studente网络)
    - light weight结构设计，例如MobileNet，SqueezeNet
  - 模型精简
    - cnn剪枝
    - 参数共享
    - 低秩分解
  - 加速计算
    - Op-level快速算法，FFT Conv2d等
    - 卷积加速计算，例如depth-wise
- 优化加速
  - NAS(神经网络架构搜索)
  - 网络并行结构合并(例如tensorRT中可以将并行结构合并计算)

2、模型移植

模型移植，跟硬件平台关系最为密切，不同芯片都会有不同部署库

- 通用ARM平台
  - [ncnn](https://github.com/Tencent/ncnn)腾讯
  - [mnn](https://github.com/alibaba/MNN)阿里
  - [paddle](https://github.com/PaddlePaddle/Paddle)百度
  - [mace](https://github.com/XiaoMi/mace)小米
  - [FeatherCNN](https://github.com/Tencent/FeatherCNN)腾讯

- 安卓手机
  - TensorFlow Lite
- 英伟达平台
  - [TensorRT](https://github.com/NVIDIA/TensorRT)
- 华为海思
  - RuyiStudio
  - MindStudio
- 安霸
  - CVFlow
- 英特尔CPU
  - OpenVINO
- 其它
  - [Tengine](https://github.com/alibaba/tengine)
  - OpenCV
  - QNNPACK

后面花篇幅，针对性总结

